{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Model 1: MLM\n",
    "\n",
    "In this notebook you will implement MLM re-ranking of the first-pass ranking retrieved from your index. \n",
    "\n",
    "Your implementation of the mixture of language models (MLM) approach should work with two fields, `title` and `content`, with weights 0.2 and 0.8, respectively. \n",
    "\n",
    "Content should be the \"catch-all\" field. Use Dirichlet smoothing with the smoothing parameter set to 2000.\n",
    "\n",
    "Be sure to use both markdown cells with section headings and explanations, as well as writing readable code, to make it clear what your intention is each step of the way through the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import pandas as pd\n",
    "import urllib,os,math\n",
    "from nltk.stem import PorterStemmer\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "es = Elasticsearch()\n",
    "es.info()\n",
    "stemmer = PorterStemmer()\n",
    "indexname = \"dbpedia_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "QUERIES2_FILE = \"data/queries2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            q=line.strip().split()\n",
    "            query=' '.join(q[1:])\n",
    "            qid=q[0]\n",
    "#             qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "            \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2 = load_queries(QUERIES2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    qrels= pd.read_csv('data\\qrels.csv')\n",
    "    \n",
    "   \n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels=load_qrels(QRELS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "FIELDS = [\"names\", \"catch_all\"]\n",
    "FIELD_WEIGHTS = [0.2, 0.8]\n",
    "LAMBDA = 0.8\n",
    "miu=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mlm(es,  qterms, doc_id):\n",
    "    score = 0  # log P(q|d)\n",
    "    \n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    # Note that global term statistics are not needed (`term_statistics=False`)\n",
    "    tv = es.termvectors(index=indexname, id=doc_id, fields=FIELDS,\n",
    "                              term_statistics=True).get(\"term_vectors\", {})\n",
    "    # compute field lengths $|d_i|$\n",
    "    len_d_i = []  # document field length\n",
    "    for i, field in enumerate(FIELDS):\n",
    "        if field in tv:\n",
    "            len_d_i.append(sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()]))\n",
    "        else:  # that document field may be empty\n",
    "            len_d_i.append(0) \n",
    "            \n",
    "    # scoring the query\n",
    "    for t in qterms:\n",
    "        Pt_theta_d = 0  # P(t|\\theta_d)\n",
    "        for i, field in enumerate(FIELDS):\n",
    "            if field in tv:\n",
    "                Ft_di = tv[field][\"terms\"].get(t, {}).get(\"term_freq\", 0)   # $P(t|d_i)$\n",
    "#                 print(Pt_di)\n",
    "            else:  # that document field is empty\n",
    "                Ft_di = 0 \n",
    "            if field in tv:\n",
    "                \n",
    "                Pt_Ci = tv[field][\"terms\"].get(t, {}).get('ttf',0) / tv[field]['field_statistics']['sum_ttf']\n",
    "            else:\n",
    "                Pt_Ci=0\n",
    "#            \n",
    "           \n",
    "#             Pt_theta_di = (1 - LAMBDA) * Pt_di + LAMBDA * Pt_Ci  # $P(t|\\theta_{d_i})$ with J-M smoothing\n",
    "            Pt_theta_di = (Ft_di + (miu * Pt_Ci))/(len_d_i[i]+miu)\n",
    "            Pt_theta_d += FIELD_WEIGHTS[i] * Pt_theta_di\n",
    "        score += Pt_theta_d  \n",
    "    \n",
    "    return score\n",
    "# score_mlm(es,['szechwan','dish','food','cuisine'],'<dbpedia:Indian_Chinese_cuisine>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced=dict(itertools.islice(queries.items(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM score  for queries 1\n",
    "scores = {}\n",
    "for qid,query in sorted(queries.items()):    \n",
    "    dscores={}\n",
    "    query1='Szechwan dish food cuisine' \n",
    "    stemmed_query=stemmer.stem(query.lower())\n",
    "    qterms=stemmed_query.split()\n",
    "        # get document IDS\n",
    "    res = es.search(indexname, q=stemmed_query, size=100)['hits']['hits']\n",
    "#     clm = CollectionLM(es, qterms)\n",
    "    for doc in res:\n",
    "        doc_id=doc['_id']\n",
    "        doc_score = score_mlm(es, qterms, doc_id)\n",
    "        if doc_score == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dscores[doc_id]=doc_score\n",
    "    scores[qid]=dscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dic = {}\n",
    "for i in scores:\n",
    "    final_dic[i] = sorted(scores[i].items(), key=lambda kv: kv[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only QueryID and EntityID\n",
    "df_list = []\n",
    "for i in final_dic:\n",
    "#     print(i)\n",
    "    for x in range(len(final_dic[i])):\n",
    "        df_list.append([i, final_dic[i][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df_list,columns=['QueryId','EntityId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/ranking_model1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM score for queries 2\n",
    "scores2 = {}\n",
    "for qid,query in sorted(queries2.items()):    \n",
    "    dscores={}\n",
    "    stemmed_query=stemmer.stem(query.lower())\n",
    "    qterms=stemmed_query.split()\n",
    "        # get document IDS\n",
    "    res = es.search(indexname, q=stemmed_query, size=100)['hits']['hits']\n",
    "#     clm = CollectionLM(es, qterms)\n",
    "    for doc in res:\n",
    "        doc_id=doc['_id']\n",
    "        doc_score = score_mlm(es, qterms, doc_id)\n",
    "        if doc_score == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dscores[doc_id]=doc_score\n",
    "    scores2[qid]=dscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dic2 = {}\n",
    "for i in scores2:\n",
    "    final_dic2[i] = sorted(scores2[i].items(), key=lambda kv: kv[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only QueryID and EntityID\n",
    "df_list2 = []\n",
    "for i in final_dic2:\n",
    "#     print(i)\n",
    "    for x in range(len(final_dic2[i])):\n",
    "        df_list2.append([i, final_dic2[i][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_prediction = pd.DataFrame(df_list2, columns=['QueryId','EntityId']).to_csv('data/ranking_model2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting rankings for the two query sets should be saved and pushed to GitHub as `data/ranking_model1.csv` and `data/ranking2_model1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
