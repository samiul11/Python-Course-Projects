{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Model 3: Your own ranking model\n",
    "\n",
    "## Team-005\n",
    "\n",
    "### Implementing FSDM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"dbpedia_text\"\n",
    "FIELDS = ['names', 'abstract', 'categories', 'similar_entities']\n",
    "\n",
    "QUERIES_1 = \"data/queries2.txt\"\n",
    "\n",
    "RANKING_FILE = \"data/first_pass_bm25_one.csv\"\n",
    "# RANKING_FILE = \"data/first_pass_bm25_two.csv\"\n",
    "\n",
    "RE_RANKED_FILE = \"data/ranking_model3.csv\"\n",
    "# RE_RANKED_FILE = \"data/ranking2_model3.csv\"\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ranking_pairs(ranking_file):\n",
    "    ranking_pairs = []\n",
    "    with open(ranking_file, 'r', encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            ranking_pairs.append(tuple(row))\n",
    "\n",
    "    return ranking_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(queries_file):\n",
    "    queries_df = pd.read_csv(queries_file, header=None, delimiter=';')\n",
    "    df = queries_df.replace(to_replace=['^\\S+\\d+\\s'], value='', regex=True)\n",
    "    queries_df = queries_df.applymap(\n",
    "        lambda x: re.findall(r'^\\S+\\d+\\s', x)[0].strip())\n",
    "    queries_df = queries_df.merge(df, right_index=True, left_index=True)\n",
    "    queries_df.columns = ['QueryId', 'Query']\n",
    "    queries_df.set_index('QueryId', inplace=True)\n",
    "\n",
    "    return queries_df['Query'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query(es, query):\n",
    "    res = es.indices.analyze(index=INDEX_NAME, body={\n",
    "        'analyzer': \"english\", 'text': query})\n",
    "    return [token['token'] for token in res.get('tokens', {})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(input_list):\n",
    "    return [b for b in zip(input_list, input_list[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_sequence(es, doc_id, field):\n",
    "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=[field])\n",
    "    # We first put terms in a position-indexed dict.\n",
    "    pos = {}\n",
    "    term_freq = {}\n",
    "\n",
    "    default_tv = {\n",
    "        'term_vectors': {\n",
    "            'field': {\n",
    "                'terms': {\n",
    "                    'word': {\n",
    "                        'term_freq': 1,\n",
    "                        'tokens': [{'position': 0}]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    tv = tv if field in tv['term_vectors'].keys() else default_tv\n",
    "    field = field if field in tv['term_vectors'].keys() else 'field'\n",
    "\n",
    "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
    "        for token in tinfo['tokens']:\n",
    "            pos[token['position']] = term\n",
    "        term_freq[term] = tinfo['term_freq']\n",
    "    # Then, turn that dict to a list.\n",
    "    seq = [None] * (max(pos.keys()) + 1)\n",
    "    for p, term in pos.items():\n",
    "        seq[p] = term\n",
    "    # Document Length\n",
    "    doc_length = len(pos.items())\n",
    "    return doc_length, term_freq, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(text, bigram):\n",
    "    \"\"\"Counts the number of bigram matches in text.\n",
    "    Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == bigram[0]:\n",
    "            if text[i + 1] == bigram[1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unordered_bigram_matches(text, bigram, w):\n",
    "    \"\"\"Counts the number of unordered bigram matches in text within a given window size.\n",
    "    Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            if other_term in text[i+1:i+w]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_language_model(totals: Series):\n",
    "    doc_len = totals['doc_length']\n",
    "    return totals.map(lambda x: x/doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_score(doc: Series, coll_lang_m: Series, smoothing_p):\n",
    "    lm_score = 0\n",
    "    for index in list(doc.index.unique()):\n",
    "        score = math.log(\n",
    "            (doc[index]+(smoothing_p*coll_lang_m[index]))/(doc['doc_length']+smoothing_p)) if coll_lang_m[index] > 0 else 0\n",
    "        lm_score = lm_score+score\n",
    "    return lm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_details(items, term_freq, term_seq, flag='unigram'):\n",
    "    entity_details = {\n",
    "        'entity': entity_id,\n",
    "        'doc_length': doc_length\n",
    "    }\n",
    "    for item in items:\n",
    "        if flag == 'unigram':\n",
    "            entity_details[item] = term_freq.get(item, 0)\n",
    "        elif flag == 'o_bigram':\n",
    "            entity_details[' '.join(list(item))] = count_ordered_bigram_matches(\n",
    "                term_seq, list(item))\n",
    "        else:\n",
    "            entity_details[' '.join(list(item))] = count_unordered_bigram_matches(\n",
    "                term_seq, list(item), w=8)\n",
    "    return entity_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(entities):\n",
    "    df = pd.DataFrame.from_records(entities)\n",
    "    df.set_index('entity', inplace=True)\n",
    "    coll_lang_model = collection_language_model(\n",
    "        df.apply(lambda x: x.sum()))\n",
    "    scores = df.apply(\n",
    "        feature_score, axis='columns', coll_lang_m=coll_lang_model, smoothing_p=2000)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_weights(row: Series, feature_weights):\n",
    "    fw = [row[index]*feature_weights[index]\n",
    "          for index in list(row.index.unique())]\n",
    "    return sum(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sdm_scores(unigram_entities, ordered_bigram_entities, unordered_bigram_entities, f_weights):\n",
    "    unigram_scores = calculate_scores(unigram_entities)\n",
    "    ordered_bigram_scores = calculate_scores(ordered_bigram_entities)\n",
    "    unordered_bigram_scores = calculate_scores(unordered_bigram_entities)\n",
    "\n",
    "    df = pd.concat([unigram_scores, ordered_bigram_scores, unordered_bigram_scores],\n",
    "                   axis=1, names=['unigram', 'ordered', 'unordered']\n",
    "                   )\n",
    "    df.columns = ['unigram', 'ordered', 'unordered']\n",
    "    return df.apply(apply_feature_weights, axis=1, feature_weights=f_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_pairs = load_ranking_pairs(RANKING_FILE)\n",
    "queries_dict = load_queries(QUERIES_1)\n",
    "\n",
    "re_ranked_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for q_id in list(queries_dict.keys()):\n",
    "    print(f\"Re-ranking entities for query: {q_id}\")\n",
    "    pairs = [pair for pair in ranking_pairs if pair[0] == q_id]\n",
    "    query = queries_dict.get(q_id, '')\n",
    "    tokens = tokenize_query(es, query)\n",
    "    bigrams = find_bigrams(tokens)\n",
    "\n",
    "    field_sdm_scores = []\n",
    "\n",
    "    for field in FIELDS:\n",
    "        unigram_entities = []\n",
    "        ordered_bigram_entities = []\n",
    "        unordered_bigram_entities = []\n",
    "\n",
    "        for _, entity_id in pairs:\n",
    "            doc_length, term_freq, term_seq = get_term_sequence(\n",
    "                es, entity_id, field)\n",
    "\n",
    "            unigram_entities.append(get_entity_details(\n",
    "                tokens, term_freq, term_seq, flag='unigram'))\n",
    "\n",
    "            ordered_bigram_entities.append(get_entity_details(\n",
    "                find_bigrams(tokens), term_freq, term_seq, flag='o_bigram'))\n",
    "\n",
    "            unordered_bigram_entities.append(get_entity_details(\n",
    "                find_bigrams(tokens), term_freq, term_seq, flag='u_bigram'))\n",
    "\n",
    "        f_weights = {\n",
    "            'unigram': 0.85,\n",
    "            'ordered': 0.1,\n",
    "            'unordered': 0.05\n",
    "        }\n",
    "        sdm_scores = calculate_sdm_scores(\n",
    "            unigram_entities, ordered_bigram_entities, unordered_bigram_entities, f_weights)\n",
    "\n",
    "        field_sdm_scores.append(sdm_scores)\n",
    "\n",
    "    fsdm_df = pd.concat(field_sdm_scores, axis=1)\n",
    "    final_scores = fsdm_df.apply(lambda x: x.sum(), axis=1)\n",
    "\n",
    "    final_scores.sort_values(ascending=False, inplace=True)\n",
    "    final_scores = final_scores.reset_index()\n",
    "    final_scores.insert(0, 'QueryId', q_id)\n",
    "    final_scores.drop(columns=[0], inplace=True)\n",
    "    final_scores.columns = ['QueryId', 'EntityId']\n",
    "    re_ranked_dfs.append(final_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_entity_df = pd.concat(re_ranked_dfs, ignore_index=True)\n",
    "query_entity_df.columns = ['QueryId', 'EntityId']\n",
    "\n",
    "with open(RE_RANKED_FILE, 'w', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    f.write(\"QueryId,EntityId\\n\")\n",
    "    for rec in query_entity_df.to_dict(orient='records'):\n",
    "        f.write(\"{},{}\\n\".format(rec['QueryId'], '\"'+rec['EntityId']+'\"'))\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\n",
    "    f\"Finished re-ranking file for {len(queries_dict.keys())} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting rankings for the two query sets should be saved and pushed to GitHub as `data/ranking_model3.csv` and `data/ranking2_model3.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
