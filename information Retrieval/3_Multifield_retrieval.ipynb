{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 3: Multifield retrieval\n",
    "\n",
    "Implement BM25F and the Mixture of Language Models (MLM). Use two fields: title and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "OUTPUT_FILE = \"data/output.txt\"  # output the ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: place the indexing related code here. This may be copy-pasted from Part 1.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk, unicodedata, re, csv, hashedindex, string, os\n",
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "from IPython.display import clear_output \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(inverse_idx):\n",
    "    TOTAL_DOC_LENGTH = 0\n",
    "    for doc in inverse_idx.documents():\n",
    "        TOTAL_DOC_LENGTH += inverse_idx.get_document_length(doc)\n",
    "    \n",
    "    AVERAGE_DOC_LENGTH =  TOTAL_DOC_LENGTH / len(inverse_idx.documents())\n",
    "    NUM_DOCS = len(inverse_idx.documents())\n",
    "    COLLECTION_DOC_LENGTH = 0 #total length of documents in the collection\n",
    "    for docID in inverse_idx.documents():\n",
    "            COLLECTION_DOC_LENGTH += inverse_idx.get_document_length(docID)\n",
    "\n",
    "    return {\n",
    "        \"AVERAGE_DOC_LENGTH\" : AVERAGE_DOC_LENGTH,\n",
    "        \"NUM_DOCS\" : NUM_DOCS,\n",
    "        \"COLLECTION_DOC_LENGTH\" : COLLECTION_DOC_LENGTH,\n",
    "        \"TOTAL_DOC_LENGTH\" : TOTAL_DOC_LENGTH\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_idx = pickle.load(open(\"data/index/indexer.p\", \"rb\" )) #file inside index is a test pickle\n",
    "inv_idx = {}\n",
    "inv_idx_stats = {}\n",
    "inv_idx['content'] = pickle.load(open(\"data/backup/indexer.p\", \"rb\" ))\n",
    "inv_idx_stats[\"content\"] = compute_stats(inv_idx['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx['title'] = pickle.load(open(\"data/backup/TitleIndexer.p\", \"rb\" ))\n",
    "inv_idx_stats[\"title\"] = compute_stats(inv_idx['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the queries from the file\n",
    "\n",
    "See the assignment description for the format of the query file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a#queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_idf(doc_containing_query_term):\n",
    "    val = 1 + ((NUM_DOCS - doc_containing_query_term + 0.5)/(doc_containing_query_term + 0.5))\n",
    "    return math.log(val)\n",
    "\n",
    "def tokenize_query_text(query):\n",
    "    clean_tokens = []\n",
    "    query = unicodedata.normalize('NFKD', query).encode('ascii', 'ignore').decode('utf-8', 'ignore') # remove non ascii\n",
    "    query = query.lower() #convert to lowercase\n",
    "    query = query.translate(str.maketrans('', '', string.punctuation)) #remove punctuations\n",
    "    for token in nltk.word_tokenize(query):\n",
    "        # skip stop words\n",
    "        if token in set(stopwords.words('english')):\n",
    "            continue\n",
    "\n",
    "        token = PorterStemmer().stem(token) # stem    \n",
    "        clean_tokens.append(token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_lm(inverse_idx,collection_doc_length):\n",
    "    CLM = {}\n",
    "    for term in inverse_idx.terms():\n",
    "        total_term_frequency = inverse_idx.get_total_term_frequency(term)\n",
    "        CLM[term] = total_term_frequency / collection_doc_length\n",
    "    \n",
    "    return CLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLM={}\n",
    "CLM['content'] = create_collection_lm(inv_idx['content'],inv_idx_stats['content']['COLLECTION_DOC_LENGTH'])\n",
    "CLM['title'] = create_collection_lm(inv_idx['title'],inv_idx_stats['title']['COLLECTION_DOC_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25F( query, k1 = 1.2, b = {\"title\":.75,\"content\":.75},number_of_results=100, field_weight = {\"title\":.7,\"content\":.3},get_score = True): \n",
    "    doc_scores = {} #stores score for each document for the provided query\n",
    "    term_score_summation = 0\n",
    "    for query_term in tokenize_query_text(query):\n",
    "        \n",
    "        for i, f_w in field_weight.items():\n",
    "            \n",
    "            if query_term in inv_idx[i].items():\n",
    "                query_term_frequency_in_doc = len(inv_idx[i][query_term])\n",
    "                idf = math.log((inv_idx_stats[i]['NUM_DOCS'] - query_term_frequency_in_doc + 0.5) / (query_term_frequency_in_doc + 0.5))\n",
    "                pseudo_term_frequency={}\n",
    "                for (doc_id, f_td) in inv_idx[i][query_term].items(): \n",
    "                    doc_length = inv_idx[i].get_document_length(doc_id)\n",
    "                    Bi = (1 - b[i] + b[i] * (doc_length/inv_idx_stats[i][\"AVERAGE_DOC_LENGTH\"]))\n",
    "                    pseudo_term_frequency[doc_id] = pseudo_term_frequency.get(doc_id, 0) + (f_w*(f_td/Bi))\n",
    "                    term_score = (pseudo_term_frequency[doc_id]/(k1 + pseudo_term_frequency[doc_id]))\n",
    "                    term_score_w_idf=round(term_score *idf,3)\n",
    "                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + term_score_w_idf\n",
    "\n",
    "    \n",
    "    sorted_list = sorted(doc_scores.items(), key=lambda score: score[1], reverse = True)[:number_of_results]\n",
    "    if not get_score:\n",
    "        relevant_articles = []\n",
    "        for x in sorted_list:\n",
    "            relevant_articles.append(x[0])\n",
    "\n",
    "        return relevant_articles\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLM_JMS(query, Lamda={\"title\":.75,\"content\":.75} ,number_of_results=100, field_weight = {\"title\":.7,\"content\":.3}):\n",
    "    doc_scores={}\n",
    "    for query_term in tokenize_query_text(query):\n",
    "        \n",
    "        for i, f_w in field_weight.items():\n",
    "            \n",
    "            if query_term in inv_idx[i].items():\n",
    "                for (doc_id, f_td) in inv_idx[i][query_term].items(): \n",
    "                    doc_length = inv_idx[i].get_document_length(doc_id)\n",
    "                    \n",
    "                    term_score = f_w*((1-Lamda[i])*(f_td / doc_length) + (Lamda[i]*CLM[i][query_term]))\n",
    "                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + term_score\n",
    "    sorted_list = sorted(doc_scores.items(), key=lambda score: score[1], reverse = True)[:number_of_results]\n",
    "    \n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(results,filename=OUTPUT_FILE):\n",
    "    with open(filename, mode='w') as index_file:\n",
    "        csv_writer = csv.writer(index_file, delimiter=',', quotechar='\"')\n",
    "        csv_writer.writerow([\"QueryId\",\"DocumentId\"])\n",
    "        for term in results:\n",
    "            for article in results[term]:\n",
    "                csv_writer.writerow([term,article]) #save as counter object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed ranking\n"
     ]
    }
   ],
   "source": [
    "results_bm25F = {}\n",
    "for q_id, query in queries.items():\n",
    "    results_bm25F[q_id] = bm25F( query, k1 = 1.2, b = {\"title\":0.4,\"content\":0.1},number_of_results=100, field_weight = {\"title\":.1,\"content\":.9},get_score = False)\n",
    "\n",
    "print(\"completed ranking\")\n",
    "save_output(results_bm25F,\"data/bm25F_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed ranking\n"
     ]
    }
   ],
   "source": [
    "results_MLM = {}\n",
    "for q_id, query in queries.items():\n",
    "    results_MLM[q_id] = MLM_JMS(query, Lamda = {\"title\":0.1,\"content\":0.1},number_of_results=100, field_weight = {\"title\":.1,\"content\":.9})\n",
    "\n",
    "print(\"completed ranking\")\n",
    "save_output(results_MLM,\"data/output_mlm_jm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Generate a ranking for each query and output the results to `OUTPUT_FILE`\n",
    "\n",
    "See the assignment description for the format of the output file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a#output-file-format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Report on the evaluation results (using the [Evaluation notebook](1_Evaluation.ipynb)) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the parameter settings used for the two methods and the method you used for exploring the parameter space.\n",
    "\n",
    "Specifically, explain how did you decide on \n",
    "  - the models' parameters ($k1$ and $b$ for BM25,smoothing method and smoothing parameter for LM);\n",
    "  - the field weights ($w_{title}$ and $w_{content}$).\n",
    "\n",
    "*TODO*\n",
    "\n",
    "Report only the best performing setting for each model in the table below. The corresponding result files should be pushed to your repository.\n",
    "\n",
    "\n",
    "| **Method** | **Parameter settings** | **Output file** | **P@10** | **MAP** | **MRR** |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| BM25F | k1: *1.2*, b: *{\"title\":0.4,\"content\":.1}*, $w_{title}$: *.1*, $w_{content}$: *.9* | `data/output_bm25f.csv` | *0.228* | *0.079* | *0.327* |      \n",
    "| MLM | Smoothing method: *jelinek merker*, smoothing param: *{\"title\":0.1,\"content\":0.1}*,$w_{title}$: *.1*, $w_{content}$: *.9* | `data/output_mlm_jm.csv` | *0.044* | *0.028* | *0.073* |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
