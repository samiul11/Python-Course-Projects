{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 2: Retrieval\n",
    "\n",
    "Implement BM25 and LM retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "OUTPUT_FILE = \"data/output.csv\"  # output the ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk, unicodedata, re, csv, hashedindex, string, os\n",
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "from IPython.display import clear_output \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = pickle.load(open(\"data/backup/indexer.p\", \"rb\" ))\n",
    "\n",
    "total_doc_length = 0\n",
    "for doc in inv_idx.documents():\n",
    "    total_doc_length += inv_idx.get_document_length(doc)\n",
    "    \n",
    "average_doc_length =  total_doc_length / len(inv_idx.documents())\n",
    "NUM_DOCS = len(inv_idx.documents())\n",
    "collection_document_length = 0 #total length of documents in the collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the queries from the file\n",
    "\n",
    "See the assignment description for the format of the query file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt - number of documents that contain t\n",
    "def bm25_idf(doc_containing_query_term):\n",
    "    val = 1 + ((NUM_DOCS - doc_containing_query_term + 0.5)/(doc_containing_query_term + 0.5))\n",
    "    return math.log(val)\n",
    "\n",
    "def tokenize_query_text(query):\n",
    "    clean_tokens = []\n",
    "    query = unicodedata.normalize('NFKD', query).encode('ascii', 'ignore').decode('utf-8', 'ignore') # remove non ascii\n",
    "    query = query.lower() #convert to lowercase\n",
    "    query = query.translate(str.maketrans('', '', string.punctuation)) #remove punctuations\n",
    "    for token in nltk.word_tokenize(query):\n",
    "        # skip stop words\n",
    "        if token in set(stopwords.words('english')):\n",
    "            continue\n",
    "\n",
    "        token = PorterStemmer().stem(token) # stem    \n",
    "        clean_tokens.append(token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(query, k = 1.2, b = 0.75, number_of_results=100, get_score = True):\n",
    "    bm25_query_doc_scores = {} #stores score for each document for the provided query\n",
    "    term_score_summation = 0\n",
    "\n",
    "    for query_term in tokenize_query_text(query):\n",
    "        #check if we have indexed this query term\n",
    "        if query_term in inv_idx.terms():\n",
    "            doc_containing_query_term = len(inv_idx.get_documents(query_term))\n",
    "            for document in inv_idx.get_documents(query_term):\n",
    "                ft_d = inv_idx.get_term_frequency(query_term,document)\n",
    "                doc_len = inv_idx.get_document_length(document)\n",
    "\n",
    "                doc_score_numerator = (ft_d * (1 + k))\n",
    "                doc_score_denomenator = (ft_d + k * (1 - b + (b * (doc_len/average_doc_length))))\n",
    "                doc_score_left = (doc_score_numerator/doc_score_denomenator)\n",
    "\n",
    "                score = (doc_score_left* bm25_idf(doc_containing_query_term))\n",
    "\n",
    "                if(document in bm25_query_doc_scores):\n",
    "                    bm25_query_doc_scores[document] += score\n",
    "                else:\n",
    "                    bm25_query_doc_scores[document] = score\n",
    "    \n",
    "    sorted_list = sorted(bm25_query_doc_scores.items(), key=lambda score: score[1], reverse = True)[:number_of_results]\n",
    "    if not get_score:\n",
    "        relevant_articles = []\n",
    "        for x in sorted_list:\n",
    "            relevant_articles.append(x[0])\n",
    "\n",
    "        return relevant_articles\n",
    "    return sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_lm():\n",
    "    CLM = {}\n",
    "    for term in inv_idx.terms():\n",
    "        total_term_frequency = inv_idx.get_total_term_frequency(term)\n",
    "        \n",
    "        CLM[term] = total_term_frequency / total_doc_length\n",
    "    \n",
    "    return CLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLM = create_collection_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_jelinek_merker_smoothing(query,Lamda=0.5, number_of_results=100, get_score=True):\n",
    "    lm_query_doc_scores = {} #stores score for each document for the provided query\n",
    "    query_list=tokenize_query_text(query)\n",
    "    for query_term in query_list:\n",
    "        f_tq=query_list.count(query_term)\n",
    "        \n",
    "        if query_term in inv_idx.terms():\n",
    "            for (doc_id, f_td) in inv_idx[query_term].items():\n",
    "                doc_length=inv_idx.get_document_length(doc_id)\n",
    "                term_score = round(math.log((1-Lamda)*(f_td / doc_length) + (Lamda*CLM[query_term])),3)\n",
    "                lm_query_doc_scores[doc_id] = lm_query_doc_scores.get(doc_id, 0) + term_score\n",
    "            lm_query_doc_scores[doc_id] = f_tq * lm_query_doc_scores[doc_id]\n",
    "    \n",
    "    sorted_list = sorted(lm_query_doc_scores.items(), key=lambda score: score[1], reverse = True)[:number_of_results]\n",
    "    if not get_score:\n",
    "        relevant_articles = []\n",
    "        for x in sorted_list:\n",
    "            relevant_articles.append(x[0])\n",
    "\n",
    "        return relevant_articles\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_dirichilet(query, number_of_results=100, get_score=True, meu = average_doc_length):\n",
    "    doc_scores = {} #stores score for each document for the provided query\n",
    "    query_list=tokenize_query_text(query)\n",
    "    for query_term in query_list:\n",
    "        f_tq=query_list.count(query_term)\n",
    "        \n",
    "        if query_term in inv_idx.terms():\n",
    "            for (doc_id, f_td) in inv_idx[query_term].items():\n",
    "                doc_length=inv_idx.get_document_length(doc_id)\n",
    "                \n",
    "                term_score = round(math.log((f_td+meu*CLM[query_term])/(doc_length + meu)),3)\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + term_score\n",
    "            doc_scores[doc_id] = f_tq * doc_scores[doc_id]\n",
    "\n",
    "    sorted_list = sorted(doc_scores.items(), key=lambda score: score[1], reverse = True)[:number_of_results]\n",
    "    if not get_score:\n",
    "        relevant_articles = []\n",
    "        for x in sorted_list:\n",
    "            relevant_articles.append(x[0])\n",
    "\n",
    "        return relevant_articles\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Generate a ranking for each query and output the results to `OUTPUT_FILE`\n",
    "\n",
    "See the assignment description for the format of the output file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(results,filename=OUTPUT_FILE):\n",
    "    with open(filename, mode='w') as index_file:\n",
    "        csv_writer = csv.writer(index_file, delimiter=',', quotechar='\"')\n",
    "        csv_writer.writerow([\"QueryId\",\"DocumentId\"])\n",
    "        for term in results:\n",
    "            for article in results[term]:\n",
    "                csv_writer.writerow([term,article]) #save as counter object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed ranking\n"
     ]
    }
   ],
   "source": [
    "# Compute BM25 and save to file\n",
    "results_bm25={}\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    results_bm25[q_id] = bm25(query, get_score=False, k=1.2, b=.3)\n",
    "    clear_output()\n",
    "print(\"completed ranking\")\n",
    "\n",
    "save_output(results_bm25, \"data/bm25_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed ranking\n"
     ]
    }
   ],
   "source": [
    "# results_LMJM\n",
    "results_LMJM={}\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    results_LMJM[q_id] = LM_jelinek_merker_smoothing(query,Lamda=0.1, number_of_results=100, get_score=False)\n",
    "print(\"completed ranking\")\n",
    "\n",
    "save_output(results_LMJM, \"data/lm_jm_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed ranking\n"
     ]
    }
   ],
   "source": [
    "results_LMD={}\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    results_LMD[q_id] = LM_dirichilet(query, number_of_results=100, get_score=False, meu = 10)\n",
    "print(\"completed ranking\")\n",
    "\n",
    "save_output(results_LMD, \"data/lm_dir_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Report on the evaluation results (using the [Evaluation notebook](1_Evaluation.ipynb)) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the parameter settings used for the two methods: *TODO*\n",
    "\n",
    "Write the name of the corresponding output file in the table. These files should be pushed to your repository.\n",
    "\n",
    "\n",
    "| **Method** | **Output file** | **P@10** | **MAP** | **MRR** |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| BM25 | `data/bm25_output.csv` | *0.220* | *0.082* | *0.356* |\n",
    "| LM | `data/lm_dir_output.csv` | *0.016* | *0.002* | *0.040* |\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
