{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 1: Indexer\n",
    "\n",
    "Index the document collection and save the index to disk.  \n",
    "\n",
    "**IMPORTANT**: The collection and index take up several hundred Megabytes. Do NOT push those to GitHub!\n",
    "\n",
    "It is recommended that you work on a small sample of documents while developing your solution. It is enough to build the full index once you get to Part 2 of the assignment, as you may realize later that certain refinements are needed.\n",
    "\n",
    "You have two main options to implement the inverted index: (1) all by yourself from scratch or (2) using the [HashedIndex](https://pypi.org/project/hashedindex/) Python library. There is no third option.\n",
    "\n",
    "You are required to adhere to the structure provided below.\n",
    "\n",
    "The code for parsing the gzip files in the collection is already given.\n",
    "\n",
    "You may decide to build two separate indices for the two document fields (title and content) or to keep them together in the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_docs_bulk(docs):\n",
    "#    for doc_id, doc in docs.items():\n",
    "#        # TODO: complete\n",
    "#        print(\"Indexing document {}\".format(doc_id))\n",
    "def add_docs_bulk(indexer, docs):\n",
    "    for doc_id, doc in docs.items():\n",
    "        indexer.add(doc[\"content\"], doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a given data file\n",
    "\n",
    "**NOTE**: Each source gzip file contains several documents. The method below does the parsing of source files and then calls `add_docs_bulk()` to bulk indexing on all document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_file(file_name,indexer):\n",
    "    print(\"Processing {0}\".format(file_name))\n",
    "    \n",
    "    with gzip.open(file_name, \"rt\") as fin:\n",
    "        is_body = False\n",
    "        docs = {}\n",
    "        doc_id, body = None, None\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<DOCNO>\"):  # get doc id\n",
    "                doc_id = re.sub(\"<DOCNO> | </DOCNO>\", \"\", line)\n",
    "            elif line.startswith(\"<BODY>\"):  # start to parse body\n",
    "                is_body = True\n",
    "                body = []\n",
    "            elif line.startswith(\"</BODY>\"):  # finished reading body\n",
    "                soup = BeautifulSoup(\"\\n\".join(body), \"lxml\")\n",
    "                headline = soup.find(\"headline\")\n",
    "                text = soup.find(\"text\")\n",
    "                docs[doc_id] = {\n",
    "                    \"title\": headline.text if headline is not None else \"\",  # use an empty string if no <HEADLINE> found\n",
    "                    \"content\": text.text if text is not None else \"\"  # everything inside <TEXT> is indexed as content\n",
    "                }\n",
    "                # get ready for next document\n",
    "                doc_id = None\n",
    "                is_body = False\n",
    "            elif is_body:  # accumulate body content\n",
    "                body.append(line)\n",
    "\n",
    "        # bulk index the collected documents\n",
    "        print(\"Bulk indexing\", len(docs), \"documents\")  \n",
    "        add_docs_bulk(indexer,docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the entire collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete (currently, indexing only a single gzip file for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_file(\"data/aquaint/nyt/2000/20000101_NYT.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Save the index to disk (make sure that the index directory is added to `.gitignore`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: install hashedindex package in conda using the following steps:\n",
    "- conda skeleton pypi hashedindex\n",
    "- conda build hashedindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk, unicodedata, re, csv, hashedindex, string, os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer():\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.index = hashedindex.HashedIndex()\n",
    "        self.document_length = {}\n",
    "\n",
    "    \"\"\"\n",
    "    text preprocessor\n",
    "    Returns word token list\n",
    "    \"\"\"\n",
    "    def text_preprocessor(self, document):\n",
    "        clean_tokens = []\n",
    "        \n",
    "        document = unicodedata.normalize('NFKD', document).encode('ascii', 'ignore').decode('utf-8', 'ignore') # remove non ascii\n",
    "        document = document.lower() #convert to lowercase\n",
    "        document = document.translate(str.maketrans('', '', string.punctuation)) #remove punctuations\n",
    "            \n",
    "        #break text into tokens\n",
    "        for token in nltk.word_tokenize(document):\n",
    "             \n",
    "            # skip stop words\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            token = self.stemmer.stem(token) # stem    \n",
    "            clean_tokens.append(token)\n",
    "            \n",
    "        return clean_tokens\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds text to the hashed index\n",
    "    \"\"\"\n",
    "    def add(self, document, docID):\n",
    "        \n",
    "        words = self.text_preprocessor(document)\n",
    "        for word in words:\n",
    "            self.index.add_term_occurrence(word, docID)\n",
    "            \n",
    "        #create a text length document when indexing a document \n",
    "        self.document_length[docID] = len(document)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    save index to file\n",
    "    Format : term, Counter({'NYT20000101.0001': 1}) <-- probably will have to change this\n",
    "    \"\"\"\n",
    "    def save_to_file(self, filename = \"data/index/Indices.csv\"):\n",
    "        with open(filename, mode='w') as index_file:\n",
    "            index_writer = csv.writer(index_file, delimiter=',', quotechar='\"')\n",
    "\n",
    "            for term in self.index.terms():\n",
    "                index_writer.writerow([term,self.index.get_documents(term)]) #save as counter object\n",
    "                #index_writer.writerow([term,list(self.index.get_documents(term).items())]) #save as list\n",
    "    \n",
    "    def save_pickle(self):\n",
    "        \n",
    "        #term_doc_dict = {}\n",
    "        #for term in self.index.terms():\n",
    "        #    term_doc_dict[term] = list(self.index.get_documents(term).items())\n",
    "                \n",
    "        #with open(\"data/doc_len.p\", 'wb') as handle:\n",
    "        #    pickle.dump(self.document_length, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        #with open(\"data/inverse_index.p\", 'wb') as handle:\n",
    "        #    pickle.dump(term_doc_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open(\"data/t--indexer.p\", 'wb') as handle:\n",
    "            pickle.dump(self.index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(\"saved to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/aquaint/nyt/2000/20000101_NYT.gz\n",
      "Bulk indexing 243 documents\n"
     ]
    }
   ],
   "source": [
    "class FileReader():\n",
    "    \n",
    "    def __init__(self,data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        self.news_sources = [\"apw\",\"nyt\",\"xie\"]\n",
    "        self.no_of_files = 0\n",
    "    \n",
    "    def indexTestFile(self,indexer):\n",
    "        test_file = \"data/aquaint/nyt/2000/20000101_NYT.gz\"\n",
    "        index_file(test_file,indexer)\n",
    "        \n",
    "    def indexAllFiles(self,indexer):\n",
    "        #i = 0\n",
    "        for news_source in self.news_sources:\n",
    "            news_dir = os.path.join(self.data_folder, news_source)\n",
    "            for subdir, dirs, files in os.walk(news_dir):\n",
    "                for file in files:\n",
    "                    if(\".gz\" in file):\n",
    "                        gz_file_path = os.path.join(subdir, file)\n",
    "                        index_file(gz_file_path,indexer)\n",
    "\n",
    "                    self.no_of_files+=1\n",
    "                    #if self.no_of_files>5:\n",
    "                    #    return\n",
    "\n",
    "indexer = Indexer()\n",
    "data_folder = \"data/aquaint/\"\n",
    "fr = FileReader(data_folder)\n",
    "fr.indexTestFile(indexer)\n",
    "# fr.indexAllFiles(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to file\n"
     ]
    }
   ],
   "source": [
    "indexer.save_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friday',\n",
       " 'april',\n",
       " '28',\n",
       " '2000',\n",
       " 'five',\n",
       " 'dead',\n",
       " 'suburban',\n",
       " 'pa',\n",
       " 'shoot',\n",
       " 'mckee']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.index.terms()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashed index pickle can be to as:\n",
    "- get documents for a word : indexer.index.get_documents('midnight')\n",
    "- get documents length : indexer.index.get_document_length(\"NYT20000101.0045\")\n",
    "- get term frequency in a doc: indexer.index.get_term_frequency(\"midnight\",\"NYT20000101.0003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.index.documents()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.index.get_document_length(\"NYT20000101.0045\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.index.get_term_frequency(\"midnight\",\"NYT20000101.0003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
