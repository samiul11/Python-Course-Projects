{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2B: Feature computation\n",
    "\n",
    "The purpose of this notebook is to perform the computation of features. \n",
    "\n",
    "Note that some features might be expensive, so you don't want to keep re-computing them. Instead, aim for writing a set of relatively simple feature extractors, each computing one or multiple features, and save their output to separate files. Then, load the pre-computed features from multiple files in the learning step (in the [ranking notebook](2_Ranking.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunnel local 5002 to the server : `ssh -N -L 5002:gustav1.ux.uis.no:5002 username@ssh1.ux.uis.no`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API = \"http://127.0.0.1:5002\"\n",
    "\n",
    "MAIN_INDEX = \"clueweb12b\"\n",
    "ANCHORS_INDEX = \"clueweb12b_anchors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "QUERY2_FILE ='data/queries2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def exists(indexname, doc_id):\n",
    "    url = \"/\".join([API, indexname, doc_id, \"_exists\"])\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)['exists']\n",
    "    \n",
    "print(exists(MAIN_INDEX, \"clueweb12-0713wb-35-00870\"))\n",
    "print(exists(MAIN_INDEX, \"clueweb12-0906wb-09-33744\"))\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = requests.get(url).text\n",
    "    r = json.loads(response)\n",
    "    return [t[\"token\"] for t in r[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_vectors(indexname, doc_id, term_statistics=False):\n",
    "    ret = {}    \n",
    "    url = \"/\".join([API, indexname, doc_id, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": str(term_statistics).lower()})\n",
    "    response = requests.get(url).text\n",
    "    try:\n",
    "        ret = json.loads(response)\n",
    "    except:\n",
    "        print(\"Failed to json-decode this response:\\n{}\".format(response))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_qlen(query, doc):\n",
    "    \"\"\"Feature: query length (number of terms). \n",
    "    This is a query feature, so it'll have the same value for all documents.\"\"\"\n",
    "    return len(query.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchByDocID(indexname, query, field, docID, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size, \"_id\":docID})\n",
    "    print(url)\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(indexname, query, field, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.31441387836048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_bm25(query, doc, field, index):\n",
    "\n",
    "    k1 = 1.2\n",
    "    b=0.75\n",
    "    \n",
    "    score = 0\n",
    "    try:\n",
    "        term_vector = term_vectors(index, doc, term_statistics=True)['term_vectors'][field]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    avg_doc_len = term_vector['field_statistics']['sum_ttf']/term_vector['field_statistics']['doc_count']\n",
    "    doc_len = sum([stats['term_freq'] for term, stats in term_vector['terms'].items()])\n",
    "\n",
    "    for term in query:\n",
    "        if term in term_vector['terms'].keys():            \n",
    "            ftd = term_vector['terms'][term]['term_freq']\n",
    "            \n",
    "            idf = math.log(term_vector['field_statistics']['doc_count']/term_vector['terms'][term]['doc_freq'])\n",
    "\n",
    "            term_score = idf*((ftd*(k1+1))/(ftd*(1-b+b*(doc_len/avg_doc_len))))\n",
    "\n",
    "            score = score + term_score\n",
    "\n",
    "    return score\n",
    "\n",
    "feature_bm25(['raspberri', 'pi'],\"clueweb12-0906wb-09-33744\",\"content\",MAIN_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.583445882212153"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_idf(query, doc, field, index):\n",
    "    score = 0\n",
    "    try:\n",
    "        term_vector = term_vectors(index, doc, term_statistics=True)['term_vectors'][field]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    avg_doc_len = term_vector['field_statistics']['sum_ttf']/term_vector['field_statistics']['doc_count']\n",
    "    doc_len = sum([stats['term_freq'] for term, stats in term_vector['terms'].items()])\n",
    "\n",
    "    for term in query:\n",
    "        if term in term_vector['terms'].keys():            \n",
    "            idf = math.log(term_vector['field_statistics']['doc_count']/term_vector['terms'][term]['doc_freq'])\n",
    "            score = score + idf\n",
    "\n",
    "    return score\n",
    "\n",
    "feature_idf(['raspberri', 'pi'],\"clueweb12-0906wb-09-33744\",\"content\",MAIN_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_docLen(doc, field, index):\n",
    "    score = 0\n",
    "    try:\n",
    "        term_vector = term_vectors(index, doc, term_statistics=True)['term_vectors'][field]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    doc_len = sum([stats['term_freq'] for term, stats in term_vector['terms'].items()])\n",
    "    return doc_len\n",
    "\n",
    "feature_docLen(\"clueweb12-0906wb-09-33744\",\"content\",MAIN_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLM_FIELDS=['title','content'] \n",
    "MLM_FIELD_WEIGHTS = [0.2, 0.8] \n",
    "MLM_LAMBDA=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionLM(object):\n",
    "    def __init__(self, qterms):\n",
    "        self._probs = {}\n",
    "        # computing P(t|C_i) for each field and for each query term\n",
    "        for field in MLM_FIELDS:\n",
    "            self._probs[field] = {}\n",
    "            for t in qterms:\n",
    "                self._probs[field][t] = self._get_prob(field, t)\n",
    "        \n",
    "    def _get_prob(self, field, term):\n",
    "        # Use a boolean query to find a document that contains the term\n",
    "        res = search(MAIN_INDEX, term, field, size=1)\n",
    "        hits = res.get('hits', {}).get(\"hits\", {})\n",
    "        doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "        if doc_id is not None:\n",
    "            # Ask for global term statistics when requesting the term vector of that doc (`term_statistics=True`)\n",
    "            # TODO: complete this part   \n",
    "            tv = term_vectors(MAIN_INDEX, doc_id, term_statistics=True)['term_vectors'][field]\n",
    "            ttf = tv['terms'].get(term, {}).get(\"ttf\", 0)  # total term count in the collection (in that field)\n",
    "            sum_ttf = tv['field_statistics']['sum_ttf']\n",
    "            return ttf / sum_ttf\n",
    "\n",
    "        return 0  # this only happens if none of the documents contain that term\n",
    "\n",
    "    def prob(self, field, term):\n",
    "        return self._probs.get(field, {}).get(term, 0)\n",
    "\n",
    "#qterms = analyze_query(MAIN_INDEX, \"ford edge problems\")\n",
    "#clm = CollectionLM(qterms)\n",
    "#clm.prob(\"content\", \"problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mlm(clm, qterms, doc_id):\n",
    "    score = 0  # log P(q|d)\n",
    "    \n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    # Note that global term statistics are not needed (`term_statistics=False`)\n",
    "    tv = term_vectors(MAIN_INDEX, doc_id, term_statistics=False).get(\"term_vectors\", {})\n",
    "    #es.termvectors(index=INDEX_NAME, id=doc_id, fields=FIELDS,\n",
    "    #                          term_statistics=False).get(\"term_vectors\", {})\n",
    "\n",
    "    # compute field lengths $|d_i|$\n",
    "    len_d_i = []  # document field length\n",
    "    for i, field in enumerate(MLM_FIELDS):\n",
    "        if field in tv: \n",
    "            len_d_i.append(sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()]))\n",
    "        else:  # that document field may be empty\n",
    "            len_d_i.append(0)\n",
    "        \n",
    "    # scoring the query\n",
    "    for t in qterms:\n",
    "        Pt_theta_d = 0  # P(t|\\theta_d)\n",
    "        for i, field in enumerate(MLM_FIELDS):\n",
    "            if field in tv:\n",
    "                Pt_di = tv[field][\"terms\"].get(t, {}).get(\"term_freq\", 0) / len_d_i[i]  # $P(t|d_i)$\n",
    "            else:  # that document field is empty\n",
    "                Pt_di = 0\n",
    "            Pt_Ci = clm.prob(field, t)  # $P(t|C_i)$\n",
    "            Pt_theta_di = (1 - MLM_LAMBDA) * Pt_di + MLM_LAMBDA * Pt_Ci  # $P(t|\\theta_{d_i})$ with J-M smoothing\n",
    "            Pt_theta_d += MLM_FIELD_WEIGHTS[i] * Pt_theta_di\n",
    "        try:\n",
    "            score += math.log(Pt_theta_d)    \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.710131249740202"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_mlm(query, docId):\n",
    "    \"\"\"Feature: MLM retrieval score\"\"\"\n",
    "    # TODO\n",
    "    #get query terms\n",
    "    qterms = analyze_query(MAIN_INDEX, query)\n",
    "    clm = CollectionLM(qterms)\n",
    "    return score_mlm(clm, qterms, docId)\n",
    "    \n",
    "    #get terms and continue\n",
    "feature_mlm(\"raspberry pi\",\"clueweb12-0906wb-09-33744\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12264890"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"get number of documents that have a given term in the given field\"\"\"\n",
    "def feature_query_matching_docs(term, docId, field):\n",
    "    res = search(MAIN_INDEX, term, field, size=1)\n",
    "    hits = res.get('hits', {}).get(\"hits\", {})\n",
    "    doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "    if doc_id is not None:  \n",
    "        tv = term_vectors(MAIN_INDEX, doc_id, term_statistics=True)['term_vectors'][field]\n",
    "        return tv['field_statistics']['doc_count']\n",
    "            \n",
    "    return 0\n",
    "\n",
    "feature_query_matching_docs(\"raspberry\",\"clueweb12-0108wb-86-18203\",\"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature computation\n",
    "\n",
    "Computes features for document-query pairs and saves them to a file.\n",
    "\n",
    "Specifically, we will save features to a JSON file, using a nested map structure, with queries on the first level, documents on the second level, and individual features on the third level. \n",
    "\n",
    "```python\n",
    "  features = {\n",
    "      'query_i': {\n",
    "          'doc_j': {\n",
    "              'feature_1': 0,  # value of feature_1 for (query_i, doc_j) pair\n",
    "              'feature_2': 0,  # value of feature_2 for (query_i, doc_j) pair\n",
    "              ...\n",
    "          }\n",
    "          ...\n",
    "      }\n",
    "      ...\n",
    "  }\n",
    "```\n",
    "\n",
    "**Note**: The set of documents for a query (for which you want to compute features) should be a combination of the documents for which you have relevance labels and the top-100 documents retrieved in first-pass retrieval.\n",
    "You can then decide in the learning part if/how you want to deal with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load actual queries from file\n",
    "#queries = [\"q1\", \"q2\", \"q3\"]\n",
    "\n",
    "features_1 = {}\n",
    "features_2 = {}\n",
    "features_3 = {}\n",
    "features_4 = {}\n",
    "features_5 = {}\n",
    "\n",
    "i = 0\n",
    "for qid, query in sorted(queries.items()):\n",
    "    i+=1\n",
    "    clear_output()\n",
    "    print(i)\n",
    "    \n",
    "    qterms = analyze_query(MAIN_INDEX, query)\n",
    "    print(qterms)\n",
    "    features_1[qid] = {}\n",
    "    features_2[qid] = {}\n",
    "    features_3[qid] = {}\n",
    "    features_4[qid] = {}\n",
    "    features_5[qid] = {}\n",
    "\n",
    "    # load document_ids from qrels file\n",
    "    # loaded document_ids are indexed\n",
    "    doc_ids = load_vaid_documents_by_qid(QRELS_FILE, qid)\n",
    "    \n",
    "    for d in doc_ids:\n",
    "        print(d)\n",
    "        # Here, two sets of features are computed in a single go to produce some toy data.\n",
    "        # Normally, you would run these sequentially.\n",
    "        #features_1[qid][d] = {\n",
    "        #    'qlen': feature_qlen(query, d)\n",
    "        #}\n",
    "        \n",
    "        #feature_bm(['raspberri', 'pi'],\"clueweb12-0906wb-09-33744\",\"content\",MAIN_INDEX)\n",
    "        #features_2[qid][d] = {\n",
    "        #    'bm25_content': feature_bm25(qterms, d, \"content\", MAIN_INDEX),\n",
    "        #    'bm25_title': feature_bm25(qterms, d, \"title\", MAIN_INDEX),\n",
    "        #    'bm25_anchor': feature_bm25(qterms, d, \"anchor\",ANCHORS_INDEX)\n",
    "        #}\n",
    "        \n",
    "        #features_3[qid][d] = {\n",
    "        #    \"mlm\" : feature_mlm(query, d)\n",
    "        #}\n",
    "        \n",
    "        #idf\n",
    "        #features_4[qid][d] = {\n",
    "        #    \"idf_title\" : feature_idf(qterms, d, \"title\",MAIN_INDEX),\n",
    "        #    \"idf_content\" : feature_idf(qterms, d, \"content\",MAIN_INDEX)\n",
    "        #}\n",
    "        \n",
    "        #docLen\n",
    "        # feature_docLen(doc, field, index)\n",
    "        features_5[qid][d] = {\n",
    "            \"docLen_title\" : feature_docLen(d, \"title\",MAIN_INDEX),\n",
    "            \"docLen_content\" : feature_docLen(d, \"content\",MAIN_INDEX),\n",
    "            \"docLen_anchor\" : feature_docLen(d, \"anchor\", ANCHORS_INDEX )\n",
    "        }\n",
    "\n",
    "\n",
    "# Write computed features to file\n",
    "#with open(\"data/features_1.json\", \"w\") as f:\n",
    "#    json.dump(features_1, f, indent=4, sort_keys=True)\n",
    "    \n",
    "#with open(\"data/features_2.json\", \"w\") as f:\n",
    "#    json.dump(features_2, f, indent=4, sort_keys=True)\n",
    "\n",
    "#with open(\"data/features_3.json\", \"w\") as f:\n",
    "#    json.dump(features_3, f, indent=4, sort_keys=True)\n",
    "\n",
    "#with open(\"data/features_4.json\", \"w\") as f:\n",
    "#    json.dump(features_4, f, indent=4, sort_keys=True)\n",
    "\n",
    "# with open(\"data/features_5.json\", \"w\") as f:\n",
    "#     json.dump(features_5, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature computation for queries2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "features_1 = {}\n",
    "features_2 = {}\n",
    "features_3 = {}\n",
    "features_4 = {}\n",
    "features_5 = {}\n",
    "\n",
    "i = 0\n",
    "for qid, query in sorted(queries2.items()):\n",
    "    i+=1\n",
    "    clear_output()\n",
    "    print(i)\n",
    "    \n",
    "    qterms = analyze_query(MAIN_INDEX, query)\n",
    "    \n",
    "    features_1[qid] = {}\n",
    "    features_2[qid] = {}\n",
    "    features_3[qid] = {}\n",
    "    features_4[qid] = {}\n",
    "\n",
    "    doc_ids = []\n",
    "    # get document IDS\n",
    "    for field in [\"content\",\"title\",\"anchor\"]:\n",
    "        \n",
    "        current_index = MAIN_INDEX\n",
    "        if(field == 'anchors'):\n",
    "            \n",
    "            current_index = ANCHORS_INDEX\n",
    "        \n",
    "        \n",
    "        res = search(current_index, ' '.join(qterms), field, size=100)['hits']['hits']\n",
    "        for doc in res:\n",
    "                if exists(MAIN_INDEX, doc['_id']) and exists(ANCHORS_INDEX, doc['_id']):\n",
    "                    doc_ids.append(doc['_id'])\n",
    "        \n",
    "        key = 'bm25_{}'.format(field)           \n",
    "        for d in doc_ids:\n",
    "            features_1[qid][d] = {}\n",
    "            features_2[qid][d] = {}\n",
    "            \n",
    "            \n",
    "            if field == \"anchor\":\n",
    "                features_2[qid][d][key] = feature_bm25(qterms, d, field, ANCHORS_INDEX)\n",
    "               \n",
    "            else:\n",
    "                features_2[qid][d][key] = feature_bm25(qterms, d, field, MAIN_INDEX)\n",
    "\n",
    "            features_1[qid][d] = {\n",
    "                'qlen': len(query.split()),\n",
    "                'q_token_len': len(qterms)\n",
    "            }\n",
    "            \n",
    "            clear_output()\n",
    "            print('done' + d)\n",
    "    \n",
    "\n",
    "# Write computed features to file\n",
    "# with open(\"data/queries2/features_1.json\", \"w\") as f:\n",
    "#     json.dump(features_1, f, indent=4, sort_keys=True)\n",
    "    \n",
    "# with open(\"data/queries2/features_2.json\", \"w\") as f:\n",
    "#     json.dump(features_2, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated under one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(given_queries):\n",
    "    features_1 = {}\n",
    "    features_2 = {}\n",
    "    features_3 = {}\n",
    "    features_4 = {}\n",
    "    features_5 = {}\n",
    "    \n",
    "    i = 0\n",
    "    for qid, query in sorted(given_queries.items()):\n",
    "        i+=1\n",
    "        clear_output()\n",
    "        print(i)\n",
    "        \n",
    "        qterms = analyze_query(MAIN_INDEX, query)\n",
    "    \n",
    "        features_1[qid] = {}\n",
    "        features_2[qid] = {}\n",
    "        features_3[qid] = {}\n",
    "        features_4[qid] = {}\n",
    "        features_5[qid] = {}\n",
    "        \n",
    "        doc_ids = []\n",
    "        # get document IDS\n",
    "        for field in [\"content\",\"title\",\"anchors\"]:\n",
    "        \n",
    "            current_index = MAIN_INDEX\n",
    "            if(field == 'anchors'):\n",
    "            \n",
    "                current_index = ANCHORS_INDEX\n",
    "        \n",
    "        \n",
    "            res = search(current_index, ' '.join(qterms), field, size=100)['hits']['hits']\n",
    "            for doc in res:\n",
    "                    if exists(MAIN_INDEX, doc['_id']) and exists(ANCHORS_INDEX, doc['_id']):\n",
    "                        doc_ids.append(doc['_id'])\n",
    "        \n",
    "         \n",
    "        for d in doc_ids:\n",
    "            features_1[qid][d] = {}\n",
    "            features_2[qid][d] = {}\n",
    "            features_3[qid][d] = {}\n",
    "            features_4[qid][d] = {}\n",
    "            features_5[qid][d] = {}\n",
    "            \n",
    "            features_1[qid][d] = {\n",
    "                'qlen': len(query.split()),\n",
    "                'q_term_len': len(qterms)\n",
    "            }\n",
    "            \n",
    "            for field in [\"content\",\"title\",\"anchors\"]:\n",
    "                \n",
    "                current_index = MAIN_INDEX\n",
    "                if(field == 'anchors'):\n",
    "                    current_index = ANCHORS_INDEX\n",
    "                    \n",
    "                key = 'bm25_{}'.format(field)\n",
    "                \n",
    "                features_2[qid][d][key] = feature_bm25(qterms, d, field, current_index)\n",
    "               \n",
    "                features_2[qid][d][key] = feature_bm25(qterms, d, field, current_index)\n",
    "                \n",
    "            features_5[qid][d] = {\n",
    "            \"docLen_title\" : feature_docLen(d, \"title\",MAIN_INDEX),\n",
    "            \"docLen_content\" : feature_docLen(d, \"content\",MAIN_INDEX),\n",
    "            \"docLen_anchor\" : feature_docLen(d, \"anchors\", ANCHORS_INDEX )\n",
    "        }\n",
    "            \n",
    "    print('finished')\n",
    "    return features_1,features_2,features_3,features_4,features_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_queries(QUERIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "feat_1,feat_2,feat_3,feat_4,feat_5 = generate_features(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2 = load_queries(QUERY2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-11fc5c6091a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_feat_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feat_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feat_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feat_4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feat_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-ac5f2f524ea8>\u001b[0m in \u001b[0;36mgenerate_features\u001b[1;34m(given_queries)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqterms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAIN_INDEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mANCHORS_INDEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-cdfe6e1ca774>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(indexname, query, field, size)\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[1;33m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"q\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"df\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda86\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda86\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda86\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "test_feat_1,test_feat_2,test_feat_3,test_feat_4,test_feat_5 = generate_features(queries2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
