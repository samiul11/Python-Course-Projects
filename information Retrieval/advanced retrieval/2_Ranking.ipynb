{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2B: Ranking\n",
    "\n",
    "This notebook contains the skeleton for training a model and then applying it to produce a document ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the precomputed features\n",
    "\n",
    "The code below loads the precomputed features and combines them into feature vectors for query-document pairs.\n",
    "\n",
    "For this part to work, you'll need to run the `1_Feature_computation` notebook first to generate the sample features JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "QUERY2_FILE ='data/queries2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "queries = load_queries(QUERIES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    labels = {}\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        for line in fin.readlines()[1:]:\n",
    "            qid, docID, relevance = line.strip().split(\",\")\n",
    "            label = qid+\"-\"+docID\n",
    "            labels[label] = int(relevance)\n",
    "\n",
    "    return labels\n",
    "\n",
    "qrels = load_qrels(QRELS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(queries, features_to_load):\n",
    "    feature_names = []\n",
    "    features = {}\n",
    "    for f in features_to_load:\n",
    "        print(\"Loading features from {}\".format(f['file']))\n",
    "        feature_names += f['features']\n",
    "        with open(f['file']) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "            for q, qdocs in data.items():\n",
    "                for d, feature_values in qdocs.items():\n",
    "                    \n",
    "                    key = \"{}-{}\".format(q, d)\n",
    "                    \n",
    "                    for feature_name in f['features']:\n",
    "                        \n",
    "                        # Note: no error checking is performed. It is assumed that all feature files\n",
    "                        # contain the same queries and documents.\n",
    "                        fvect = features.get(key, [])\n",
    "                        \n",
    "                        fvect.append(feature_values[feature_name])\n",
    "                        \n",
    "                        features[key] = fvect\n",
    "\n",
    "    print(\"Feature vector: {}\".format(feature_names))\n",
    "    new_feature = {}\n",
    "    for key,value in features.items():\n",
    "        if sum(value[1:]) > 0:\n",
    "            new_feature[key] = value\n",
    "    return new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from data/features_1.json\n",
      "Loading features from data/features_2.json\n",
      "Loading features from data/features_4.json\n",
      "Loading features from data/features_5.json\n",
      "Loading features from data/features_6.json\n",
      "Feature vector: ['qlen', 'bm25_title', 'bm25_content', 'idf_title', 'idf_content', 'docLen_title', 'docLen_content', 'docLen_anchor', 'pagerank']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 31.36776352363769,\n",
       " 30.951880129983103,\n",
       " 18.170072596082754,\n",
       " 11.587501074035565,\n",
       " 9,\n",
       " 3355,\n",
       " 0,\n",
       " 5.2002362647602316e-08]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the features to be loaded from each file.\n",
    "# They will make up the feature vector in this exact order.\n",
    "features_to_load = [\n",
    "    {\n",
    "        'file': \"data/features_1.json\",\n",
    "        'features': [\"qlen\"]\n",
    "    },\n",
    "    {\n",
    "        'file': \"data/features_2.json\",\n",
    "        'features': [\"bm25_title\", \"bm25_content\"]\n",
    "    },\n",
    "    #{\n",
    "    #    'file': \"data/features_3.json\",\n",
    "    #    'features': [\"mlm\"]\n",
    "    #},\n",
    "    {\n",
    "        'file': \"data/features_4.json\", \n",
    "        'features': [\"idf_title\", \"idf_content\"]\n",
    "    } ,\n",
    "    {\n",
    "        'file': \"data/features_5.json\", \n",
    "        'features': [\"docLen_title\", \"docLen_content\", \"docLen_anchor\"]\n",
    "    },\n",
    "    {\n",
    "        'file': \"data/features_6.json\", \n",
    "        'features': [\"pagerank\"]\n",
    "    } \n",
    "]\n",
    "features = load_features(queries, features_to_load)\n",
    "NO_OF_FEATURES = features\n",
    "list(features.items())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the feature vectors look like\n",
    "# for k, v in features.items():\n",
    "#     print(k, v)\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Training needs to be done differently based on the scenario:\n",
    "\n",
    "  * **Scenario 1**: The model is trained using cross-validation, that is on 4/5 of queries, then applied on the remaining 1/5 of queries (repeated 5 times).\n",
    "  * **Scenario 2**: The model is trained on all available training data.\n",
    "  \n",
    "The feature vectors at this point are already created. These should contain both (a) the training queries and (b) the queries on which you want to apply your model.\n",
    "\n",
    "Train your model on queries (a). For that you'll also need to load the corresponding relevance labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log(i + 1, 2)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "\n",
    "\n",
    "def load_qrelsdata(file):\n",
    "    gtruth = {}\n",
    "    with open(file, \"r\") as fin:\n",
    "        fin.readline()  # excluding header line from processing\n",
    "        for line in fin:\n",
    "            qid, did, rel = line.strip().split(\",\", 2)\n",
    "            if not qid in gtruth:\n",
    "                gtruth[qid] = {}\n",
    "            gtruth[qid][did] = int(rel)\n",
    "    return gtruth\n",
    "qrels = load_qrelsdata(QRELS_FILE)\n",
    "\n",
    "def eval_scores(rankings):\n",
    "    sum_ndcg5 = 0\n",
    "    sum_ndcg10 = 0\n",
    "    sum_ndcg20 = 0\n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = qrels[qid]    \n",
    "\n",
    "        gains = [] # holds corresponding relevance levels for the ranked docs\n",
    "        for doc_id,score in ranking: \n",
    "            gain = gt.get(doc_id, 0)\n",
    "            gains.append(gain)\n",
    "\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "\n",
    "        ndcg5 = dcg(gains, 5) / dcg(gain_ideal, 5)\n",
    "        ndcg10 = dcg(gains, 10) / dcg(gain_ideal, 10)\n",
    "        ndcg20 = dcg(gains, 20) / dcg(gain_ideal, 20)\n",
    "        sum_ndcg5 += ndcg5\n",
    "        sum_ndcg10 += ndcg10\n",
    "        sum_ndcg20 += ndcg20\n",
    "    return {\n",
    "        \"ndcg@5\" : round(sum_ndcg5 / len(rankings), 3), \n",
    "        \"ndcg@10\": round(sum_ndcg10 / len(rankings), 3),\n",
    "        \"ndcg@20\": round(sum_ndcg20 / len(rankings), 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(features,qrels):\n",
    "    X, Y, qids, doc_ids = [], [], [], []\n",
    "    for label,feat in features.items():\n",
    "        qdid=label.split('-')\n",
    "        qid=qdid[0]\n",
    "        doc_id = \"{}-{}-{}-{}\".format(qdid[1], qdid[2],qdid[3],qdid[4])\n",
    "        qids.append(qid)\n",
    "        doc_ids.append(doc_id)\n",
    "        X.append(feat)\n",
    "        Y.append(qrels.get(label,0))\n",
    "    feature_vector = X\n",
    "    mm_scaler = preprocessing.MinMaxScaler()\n",
    "    feature_vector = mm_scaler.fit_transform(feature_vector)\n",
    "    return feature_vector, Y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_qids):\n",
    "    train_X, train_y = [], []\n",
    "    for i in range(len(X)):\n",
    "        if qids[i] in train_qids:\n",
    "            train_X.append(X[i])\n",
    "            train_y.append(Y[i])\n",
    "    \n",
    "    return train_X,train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_by_qid(qrels_file,query_id):\n",
    "    docs = []\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        for line in fin.readlines()[1:]:\n",
    "            qid, docID, query = line.strip().split(\",\")\n",
    "            if(qid == query_id):\n",
    "                docs.append(docID)\n",
    "    return list(set(docs))\n",
    "\n",
    "def get_features_docs_by_qid(qid):\n",
    "    test_doc_ids = load_documents_by_qid(QRELS_FILE,qid)\n",
    "    test_X = []\n",
    "\n",
    "    for d in test_doc_ids:\n",
    "        key = \"{}-{}\".format(qid, d)\n",
    "        if key in list(features.keys()):\n",
    "            test_X.append(features[key])\n",
    "        else:\n",
    "            test_X.append([-1])\n",
    "    \n",
    "    return test_X, test_doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, qids, doc_ids=load_data(features,qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samiul\\Anaconda86\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "qids_unique= list(set(qids))\n",
    "\n",
    "train_qids = []\n",
    "test_qids = []\n",
    "\n",
    "rankings = {}\n",
    "clf = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "\n",
    "# split the data into test train set\n",
    "for train_index, test_index in cv.split(qids_unique):\n",
    "\n",
    "    train_qids = [qids_unique[i] for i in train_index]\n",
    "    test_qids = [qids_unique[i] for i in test_index]\n",
    "\n",
    "    train_X, train_Y = get_train_data(train_qids)\n",
    "    \n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    \n",
    "    # train with train split\n",
    "    ltr._train(train_X, train_Y)\n",
    "    \n",
    "    # generate ranking for validation fold\n",
    "    for test_qid in test_qids:\n",
    "        test_X, test_doc_ids = get_features_docs_by_qid(test_qid)\n",
    "        \n",
    "        ranks = ltr.rank(test_X, test_doc_ids)\n",
    "        rankings[test_qid] = sorted(ranks, key=lambda score: score[1], reverse = True)[:100]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@5': 0.131, 'ndcg@10': 0.144, 'ndcg@20': 0.168}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_scores(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model to produce a ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the train model on queries (b) and sort documents according to the predicted relevance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use all of query 1 as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the features to be loaded from each file.\n",
    "# They will make up the feature vector in this exact order.\n",
    "features_to_load_query2 = [\n",
    "    {\n",
    "        'file': \"data/queries/features_1.json\",\n",
    "        'features': [\"qlen\"]\n",
    "    },  # feature 1\n",
    "    {\n",
    "        'file': \"data/queries/features_2.json\",\n",
    "        'features': [\"bm25_title\", \"bm25_content\"]\n",
    "    },  # feature 2, feature 3\n",
    "    {\n",
    "        'file': \"data/queries/features_3.json\",\n",
    "        'features': [\"mlm\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from data/features_1.json\n",
      "Loading features from data/features_2.json\n",
      "Feature vector: ['qlen', 'bm25_title', 'bm25_content']\n"
     ]
    }
   ],
   "source": [
    "queries2 = load_queries(QUERY2_FILE)\n",
    "features2 = load_features(queries, features_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr = PointWiseLTRModel(clf)\n",
    "\n",
    "# get training data\n",
    "train_X, train_Y = get_train_data(qids_unique)\n",
    "\n",
    "ltr._train(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, _, _=load_data(features,qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(features2, queries2, ltr):\n",
    "    # output_format = \"trec 22\"\n",
    "    OUTPUT_FILE = \"data/queries2_ranking.csv\"\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\") as fout:\n",
    "        fout.write(\"QueryId,DocumentId\\n\")\n",
    "        for qid, query in sorted(queries2.items()):\n",
    "            # Convert into the format required by the `PointWiseLTRModel` class\n",
    "            # and deal with missing feature values\n",
    "            features, _, qids, doc_ids=load_data(features2,qrels)\n",
    "            \n",
    "            r = ltr.rank(features, doc_ids)\n",
    "            # Write the results to file\n",
    "            rank = 1\n",
    "            for doc_id, score in r:\n",
    "                if rank <= 20:\n",
    "                    fout.write(qid + \",\" + doc_id + \"\\n\")\n",
    "                    rank += 1\n",
    "                else:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result(features2, queries2, ltr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
